{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4476db4f-fdc7-4fcb-9cb0-e616c18ac19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-01 04:33:55 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Running on Apache Spark version 3.1.3\n",
      "SparkUI available at http://3c3bb513ee25:4040\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.98-f8833c1ae16b\n",
      "LOGGING: writing to /home/jupyter/hail-20220901-0433-0.2.98-f8833c1ae16b.log\n",
      "2022-09-01 04:46:02 Hail: INFO: wrote matrix table with 1115684 rows and 955359 columns in 250 partitions to gnomad_v4.0_test.vds/reference_data\n",
      "2022-09-01 04:50:46 Hail: INFO: wrote matrix table with 986994 rows and 955359 columns in 250 partitions to gnomad_v4.0_test.vds/variant_data\n"
     ]
    }
   ],
   "source": [
    "# LT 01/09/2022\n",
    "# create a gnomAD v4 small test vds with 0.5% partitions\n",
    "\n",
    "import hail as hl\n",
    "hl.init(default_reference='GRCh38',\n",
    "        spark_conf = {\n",
    "            'spark.hadoop.fs.gs.requester.pays.mode': 'CUSTOM',\n",
    "            'spark.hadoop.fs.gs.requester.pays.buckets': 'gnomad',\n",
    "            'spark.hadoop.fs.gs.requester.pays.project.id': 'constraint-232598',\n",
    "        })\n",
    "\n",
    "# gnomAD v4 exomes VDS\n",
    "big_vds = hl.vds.read_vds('gs://gnomad/v4.0/raw/exomes/gnomad_v4.0.vds')\n",
    "\n",
    "# reduce to one single partition for testing\n",
    "small_vds = hl.vds.VariantDataset(\n",
    "    big_vds.reference_data._filter_partitions(range(250)),\n",
    "    big_vds.variant_data._filter_partitions(range(250)),\n",
    ")\n",
    "small_vds.write('gnomad_v4.0_test.vds', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c76840db-1eae-40b2-bd61-519f75315c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-07 04:44:44 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Running on Apache Spark version 3.1.3\n",
      "SparkUI available at http://edaca21c8e23:4040\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.98-f8833c1ae16b\n",
      "LOGGING: writing to /home/jupyter/hail-20220907-0444-0.2.98-f8833c1ae16b.log\n"
     ]
    }
   ],
   "source": [
    "import hail as hl\n",
    "hl.init(default_reference='GRCh38',\n",
    "        spark_conf = {\n",
    "            'spark.hadoop.fs.gs.requester.pays.mode': 'CUSTOM',\n",
    "            'spark.hadoop.fs.gs.requester.pays.buckets': 'gnomad',\n",
    "            'spark.hadoop.fs.gs.requester.pays.project.id': 'constraint-232598',\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d5d381b-f109-4eeb-92dd-d51c76ec3a11",
   "metadata": {},
   "outputs": [
    {
     "ename": "FatalError",
     "evalue": "GoogleJsonResponseException: 400 Bad Request\n{\n  \"code\" : 400,\n  \"errors\" : [ {\n    \"domain\" : \"global\",\n    \"message\" : \"Bucket is a requester pays bucket but no user project provided.\",\n    \"reason\" : \"required\"\n  } ],\n  \"message\" : \"Bucket is a requester pays bucket but no user project provided.\"\n}\n\nJava stack trace:\njava.io.IOException: Error accessing gs://gnomad-public-requester-pays/ressources/context/grch38_context_vep_annotated.v101.ht\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:1911)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:1813)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1127)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1095)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:1038)\n\tat is.hail.io.fs.HadoopFS.fileStatus(HadoopFS.scala:166)\n\tat is.hail.io.fs.FS.isDir(FS.scala:364)\n\tat is.hail.io.fs.FS.isDir$(FS.scala:362)\n\tat is.hail.io.fs.HadoopFS.isDir(HadoopFS.scala:72)\n\tat is.hail.expr.ir.RelationalSpec$.readMetadata(AbstractMatrixTableSpec.scala:31)\n\tat is.hail.expr.ir.RelationalSpec$.readReferences(AbstractMatrixTableSpec.scala:74)\n\tat is.hail.variant.ReferenceGenome$.fromHailDataset(ReferenceGenome.scala:581)\n\tat is.hail.variant.ReferenceGenome.fromHailDataset(ReferenceGenome.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\ncom.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n  \"code\" : 400,\n  \"errors\" : [ {\n    \"domain\" : \"global\",\n    \"message\" : \"Bucket is a requester pays bucket but no user project provided.\",\n    \"reason\" : \"required\"\n  } ],\n  \"message\" : \"Bucket is a requester pays bucket but no user project provided.\"\n}\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:150)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:451)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1089)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:549)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:482)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:599)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:1905)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:1813)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1127)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1095)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:1038)\n\tat is.hail.io.fs.HadoopFS.fileStatus(HadoopFS.scala:166)\n\tat is.hail.io.fs.FS.isDir(FS.scala:364)\n\tat is.hail.io.fs.FS.isDir$(FS.scala:362)\n\tat is.hail.io.fs.HadoopFS.isDir(HadoopFS.scala:72)\n\tat is.hail.expr.ir.RelationalSpec$.readMetadata(AbstractMatrixTableSpec.scala:31)\n\tat is.hail.expr.ir.RelationalSpec$.readReferences(AbstractMatrixTableSpec.scala:74)\n\tat is.hail.variant.ReferenceGenome$.fromHailDataset(ReferenceGenome.scala:581)\n\tat is.hail.variant.ReferenceGenome.fromHailDataset(ReferenceGenome.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\n\n\nHail version: 0.2.98-f8833c1ae16b\nError summary: GoogleJsonResponseException: 400 Bad Request\n{\n  \"code\" : 400,\n  \"errors\" : [ {\n    \"domain\" : \"global\",\n    \"message\" : \"Bucket is a requester pays bucket but no user project provided.\",\n    \"reason\" : \"required\"\n  } ],\n  \"message\" : \"Bucket is a requester pays bucket but no user project provided.\"\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFatalError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# gnomAD context file VEP v101\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ht \u001b[38;5;241m=\u001b[39m \u001b[43mhl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgs://gnomad-public-requester-pays/ressources/context/grch38_context_vep_annotated.v101.ht\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<decorator-gen-1387>:2\u001b[0m, in \u001b[0;36mread_table\u001b[0;34m(path, _intervals, _filter_intervals, _n_partitions, _assert_type, _load_refs)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/python310/lib/python3.10/site-packages/hail/typecheck/check.py:577\u001b[0m, in \u001b[0;36m_make_dec.<locals>.wrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;129m@decorator\u001b[39m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(__original_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    576\u001b[0m     args_, kwargs_ \u001b[38;5;241m=\u001b[39m check_all(__original_func, args, kwargs, checkers, is_method\u001b[38;5;241m=\u001b[39mis_method)\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__original_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/python310/lib/python3.10/site-packages/hail/methods/impex.py:2925\u001b[0m, in \u001b[0;36mread_table\u001b[0;34m(path, _intervals, _filter_intervals, _n_partitions, _assert_type, _load_refs)\u001b[0m\n\u001b[1;32m   2913\u001b[0m \u001b[38;5;124;03m\"\"\"Read in a :class:`.Table` written with :meth:`.Table.write`.\u001b[39;00m\n\u001b[1;32m   2914\u001b[0m \n\u001b[1;32m   2915\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2922\u001b[0m \u001b[38;5;124;03m:class:`.Table`\u001b[39;00m\n\u001b[1;32m   2923\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _load_refs:\n\u001b[0;32m-> 2925\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rg_config \u001b[38;5;129;01min\u001b[39;00m \u001b[43mEnv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_references_from_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   2926\u001b[0m         hl\u001b[38;5;241m.\u001b[39mReferenceGenome\u001b[38;5;241m.\u001b[39m_from_config(rg_config)\n\u001b[1;32m   2928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _intervals \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m _n_partitions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/python310/lib/python3.10/site-packages/hail/backend/spark_backend.py:328\u001b[0m, in \u001b[0;36mSparkBackend.load_references_from_dataset\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_references_from_dataset\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhail_package\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReferenceGenome\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromHailDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/envs/python310/lib/python3.10/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/conda/envs/python310/lib/python3.10/site-packages/hail/backend/py4j_backend.py:31\u001b[0m, in \u001b[0;36mhandle_java_exception.<locals>.deco\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     tpl \u001b[38;5;241m=\u001b[39m Env\u001b[38;5;241m.\u001b[39mjutils()\u001b[38;5;241m.\u001b[39mhandleForPython(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m     30\u001b[0m     deepest, full, error_id \u001b[38;5;241m=\u001b[39m tpl\u001b[38;5;241m.\u001b[39m_1(), tpl\u001b[38;5;241m.\u001b[39m_2(), tpl\u001b[38;5;241m.\u001b[39m_3()\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m fatal_error_from_java_error_triplet(deepest, full, error_id) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pyspark\u001b[38;5;241m.\u001b[39msql\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mCapturedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FatalError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mJava stack trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     34\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHail version: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     35\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError summary: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (e\u001b[38;5;241m.\u001b[39mdesc, e\u001b[38;5;241m.\u001b[39mstackTrace, hail\u001b[38;5;241m.\u001b[39m__version__, e\u001b[38;5;241m.\u001b[39mdesc)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mFatalError\u001b[0m: GoogleJsonResponseException: 400 Bad Request\n{\n  \"code\" : 400,\n  \"errors\" : [ {\n    \"domain\" : \"global\",\n    \"message\" : \"Bucket is a requester pays bucket but no user project provided.\",\n    \"reason\" : \"required\"\n  } ],\n  \"message\" : \"Bucket is a requester pays bucket but no user project provided.\"\n}\n\nJava stack trace:\njava.io.IOException: Error accessing gs://gnomad-public-requester-pays/ressources/context/grch38_context_vep_annotated.v101.ht\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:1911)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:1813)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1127)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1095)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:1038)\n\tat is.hail.io.fs.HadoopFS.fileStatus(HadoopFS.scala:166)\n\tat is.hail.io.fs.FS.isDir(FS.scala:364)\n\tat is.hail.io.fs.FS.isDir$(FS.scala:362)\n\tat is.hail.io.fs.HadoopFS.isDir(HadoopFS.scala:72)\n\tat is.hail.expr.ir.RelationalSpec$.readMetadata(AbstractMatrixTableSpec.scala:31)\n\tat is.hail.expr.ir.RelationalSpec$.readReferences(AbstractMatrixTableSpec.scala:74)\n\tat is.hail.variant.ReferenceGenome$.fromHailDataset(ReferenceGenome.scala:581)\n\tat is.hail.variant.ReferenceGenome.fromHailDataset(ReferenceGenome.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\ncom.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n  \"code\" : 400,\n  \"errors\" : [ {\n    \"domain\" : \"global\",\n    \"message\" : \"Bucket is a requester pays bucket but no user project provided.\",\n    \"reason\" : \"required\"\n  } ],\n  \"message\" : \"Bucket is a requester pays bucket but no user project provided.\"\n}\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:150)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:451)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1089)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:549)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:482)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:599)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:1905)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:1813)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1127)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1095)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:1038)\n\tat is.hail.io.fs.HadoopFS.fileStatus(HadoopFS.scala:166)\n\tat is.hail.io.fs.FS.isDir(FS.scala:364)\n\tat is.hail.io.fs.FS.isDir$(FS.scala:362)\n\tat is.hail.io.fs.HadoopFS.isDir(HadoopFS.scala:72)\n\tat is.hail.expr.ir.RelationalSpec$.readMetadata(AbstractMatrixTableSpec.scala:31)\n\tat is.hail.expr.ir.RelationalSpec$.readReferences(AbstractMatrixTableSpec.scala:74)\n\tat is.hail.variant.ReferenceGenome$.fromHailDataset(ReferenceGenome.scala:581)\n\tat is.hail.variant.ReferenceGenome.fromHailDataset(ReferenceGenome.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\n\n\nHail version: 0.2.98-f8833c1ae16b\nError summary: GoogleJsonResponseException: 400 Bad Request\n{\n  \"code\" : 400,\n  \"errors\" : [ {\n    \"domain\" : \"global\",\n    \"message\" : \"Bucket is a requester pays bucket but no user project provided.\",\n    \"reason\" : \"required\"\n  } ],\n  \"message\" : \"Bucket is a requester pays bucket but no user project provided.\"\n}"
     ]
    }
   ],
   "source": [
    "# gnomAD context file VEP v101\n",
    "ht = hl.read_table('gs://gnomad-public-requester-pays/ressources/context/grch38_context_vep_annotated.v101.ht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58812cce-48c5-4394-b927-1381178089ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnomAD v4 exomes VDS\n",
    "big_vds = hl.vds.read_vds('gs://gnomad/v4.0/raw/exomes/gnomad_v4.0.vds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a79ec2-4022-4210-bfba-29ff72718726",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python310]",
   "language": "python",
   "name": "conda-env-python310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
